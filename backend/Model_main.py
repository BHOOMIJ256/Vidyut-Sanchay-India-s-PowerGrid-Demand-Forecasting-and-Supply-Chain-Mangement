# -*- coding: utf-8 -*-
"""Copy of exp1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11HfonAjZCMmRZxfFQs0KZJhx9HzjZqy7
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Train-test split
from sklearn.model_selection import train_test_split

# Model (you can replace with ExtraTreesRegressor or CatBoost)
from sklearn.ensemble import ExtraTreesRegressor

# Metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

df = pd.read_csv(r"/content/power_grid_data_100k.csv")

df.shape

df.head()

df = df.drop(columns=['JCB'])

df.shape

print(df.info())

df.describe()

df.duplicated().sum()

"""Univariate analysis"""

print("Value Counts:")
print(df['project_type'].value_counts())
print("\nPercentage:")
print(df['project_type'].value_counts(normalize=True) * 100)

# 2. Bar Plot (BEST for categorical)
plt.figure(figsize=(10,5))
sns.countplot(data=df, x='project_type', order=df['project_type'].value_counts().index)
plt.xticks(rotation=45)
plt.title(f"Distribution of {'project_type'}")
plt.xlabel('project_type')
plt.ylabel("Count")
plt.tight_layout()
plt.show()

plt.figure(figsize=(7,7))
df['region'].value_counts().plot.pie(autopct='%1.1f%%', shadow=False)
plt.title(f"Percentage Distribution of {'region'}")
plt.ylabel("")  # hides the side label
plt.show()

# 2. Bar Plot (BEST for categorical)
column = 'soil_type'

plt.figure(figsize=(10,5))
sns.countplot(data=df, x=column, order=df[column].value_counts().index)
plt.xticks(rotation=45)
plt.title(f"Distribution of {column}")
plt.xlabel(column)
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# 2. Bar Plot (BEST for categorical)
column = 'terrain_type'

plt.figure(figsize=(10,5))
sns.countplot(data=df, x=column, order=df[column].value_counts().index)
plt.xticks(rotation=45)
plt.title(f"Distribution of {column}")
plt.xlabel(column)
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# 2. Bar Plot (BEST for categorical)
column = 'state'

plt.figure(figsize=(13,7))
sns.countplot(data=df, x=column, order=df[column].value_counts().index)
plt.xticks(rotation=45)
plt.title(f"Distribution of {column}")
plt.xlabel(column)
plt.ylabel("Count")
plt.tight_layout()
plt.show()

"""Dropping the state , project_name , start_date columns as those are not usefull to Machine learning"""

df = df.drop(columns=['state','project_name','start_date'])

target_cols = ['steel_tonnes',	'conductor_km',	'insulators_unit' ,	'concrete_cubic_meter','Bus_reactor','Transformers','circuit_breaker'
]

for col in target_cols:
    corr = df['planned_duration_months'].corr(df[col])
    print(f"Correlation with {col}: {corr}")

"""As We can see there is very weak relationship between the planned_duration_months and output features
- so we should drop the feature
"""

df = df.drop(columns=['planned_duration_months'])

df.shape

plt.figure(figsize=(8,4))
sns.histplot(df['voltage_kv'], kde=True)
plt.title(f"Distribution of voltage_kv")
plt.xlabel('voltage_kv')
plt.ylabel("Frequency")
plt.show()

"""Mutlivariate analysis"""

target_cols = ['steel_tonnes',	'conductor_km',	'insulators_unit' ,	'concrete_cubic_meter','Bus_reactor','Transformers','circuit_breaker'
]

for col in target_cols:
    corr = df['Length_km'].corr(df[col])
    print(f"Correlation with {col}: {corr}")

target_cols = ['steel_tonnes',	'conductor_km',	'insulators_unit' ,	'concrete_cubic_meter','Bus_reactor','Transformers','circuit_breaker'
]

for col in target_cols:
    corr = df['num_towers'].corr(df[col])
    print(f"Correlation with {col}: {corr}")

target_cols = ['steel_tonnes',	'conductor_km',	'insulators_unit' ,	'concrete_cubic_meter','Bus_reactor','Transformers','circuit_breaker'
]

for col in target_cols:
    corr = df['steel_price_index'].corr(df[col])
    print(f"Correlation with {col}: {corr}")

target_cols = ['steel_tonnes',	'conductor_km',	'insulators_unit' ,	'concrete_cubic_meter','Bus_reactor','Transformers','circuit_breaker'
]

for col in target_cols:
    corr = df['conductor_price_index'].corr(df[col])
    print(f"Correlation with {col}: {corr}")

target_cols = ['steel_tonnes',	'conductor_km',	'insulators_unit' ,	'concrete_cubic_meter','Bus_reactor','Transformers','circuit_breaker'
]

for col in target_cols:
    corr = df['fuel_price_index'].corr(df[col])
    print(f"Correlation with {col}: {corr}")

target_cols = ['steel_tonnes',	'conductor_km',	'insulators_unit' ,	'concrete_cubic_meter','Bus_reactor','Transformers','circuit_breaker'
]

for col in target_cols:
    corr = df['inflation_rate'].corr(df[col])
    print(f"Correlation with {col}: {corr}")

"""so I checked correlation between the some input features and output featues, as I had doubt on it
- so I have understood that these input feature has no-correlation with the output feature, so it is best to drop it.

"""

df = df.drop(columns=['inflation_rate','steel_price_index','conductor_price_index','fuel_price_index'])

df.shape

df.head()

df.to_csv("without_OHE_preprocessed_data_power_grid_100k.csv", index=False)

"""<h2>Preprocessing </h2>"""

#Missing value
df.isnull().sum().sum()

#Handling the categorical
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

OHE_project_type = OneHotEncoder(sparse_output=False)

encoded = OHE_project_type.fit_transform(df[['project_type']])
encoded_df = pd.DataFrame(encoded, columns=OHE_project_type.get_feature_names_out(['project_type']))

df = pd.concat([df.drop(columns=['project_type']), encoded_df], axis=1)

import joblib
joblib.dump(OHE_project_type, "OHE_project_type.pkl")

df.head()

# Select column
region_col = df[['region']]

# OHE encoder
OHE_region = OneHotEncoder(sparse_output=False, drop=None)

region_encoded = OHE_region.fit_transform(region_col)

# Convert to DataFrame
region_df = pd.DataFrame(region_encoded, columns=OHE_region.get_feature_names_out(['region']))

# Add back and drop original
df = pd.concat([df.drop(columns=['region']), region_df], axis=1)

df.head()

joblib.dump(OHE_region , "OHE_region.pkl")

# Select column
soil_col = df[['soil_type']]

# OHE encoder
OHE_soil = OneHotEncoder(sparse_output=False, drop=None)

soil_encoded = OHE_soil.fit_transform(soil_col)

# Convert to DataFrame
soil_df = pd.DataFrame(soil_encoded, columns=OHE_soil.get_feature_names_out(['soil_type']))

# Add back and drop original
df = pd.concat([df.drop(columns=['soil_type']), soil_df], axis=1)

df.head()

joblib.dump(OHE_soil , "OHE_soil.pkl")

df.columns

# Select column
terrain_col = df[['terrain_type']]

# OHE encoder
OHE_terrain = OneHotEncoder(sparse_output=False, drop=None, handle_unknown='ignore')

terrain_encoded = OHE_terrain.fit_transform(terrain_col)

# Convert to DataFrame
terrain_df = pd.DataFrame(terrain_encoded, columns=OHE_terrain.get_feature_names_out(['terrain_type']))

# Add back and drop original
df = pd.concat([df.drop(columns=['terrain_type']), terrain_df], axis=1)

df.head()

type(OHE_terrain)

joblib.dump(OHE_terrain,"terrain_encoded.pkl")

# Select column
circuit_col = df[['circuit_type']]

# OHE encoder
OHE_circuit = OneHotEncoder(sparse_output=False, drop=None)

# Fit + transform
circuit_encoded = OHE_circuit.fit_transform(circuit_col)

# Convert to DataFrame
circuit_df = pd.DataFrame(
    circuit_encoded,
    columns=OHE_circuit.get_feature_names_out(['circuit_type'])
)

# Add back and drop original
df = pd.concat([df.drop(columns=['circuit_type']), circuit_df], axis=1)

df.head()

joblib.dump(OHE_circuit,"OHE_circuit.pkl")

# Select column
conductor_col = df[['conductor_type']]

# OHE encoder
OHE_conductor = OneHotEncoder(sparse_output=False, drop=None)

# Fit + transform
conductor_encoded = OHE_conductor.fit_transform(conductor_col)

# Convert to DataFrame
conductor_df = pd.DataFrame(
    conductor_encoded,
    columns=OHE_conductor.get_feature_names_out(['conductor_type'])
)

# Add back and drop original
df = pd.concat([df.drop(columns=['conductor_type']), conductor_df], axis=1)

df.head()

joblib.dump(OHE_conductor,'OHE_conductor.pkl')

df.info()

from scipy.stats import shapiro, probplot

# Columns to analyze
cols = ["voltage_kv", "Length_km", "num_towers"]

for col in cols:
    print(f"\n\n================= {col} =================")

    # 1. Skewness
    skew_val = df[col].skew()
    print(f"Skewness: {skew_val:.4f}")

    # 2. Shapiro-Wilk Test (normality test)
    stat, p = shapiro(df[col].dropna())
    print(f"Shapiro Test --> Stat: {stat:.4f}, p-value: {p:.4f}")
    if p > 0.05:
        print("=> Data seems normally distributed (fail to reject H0)")
    else:
        print("=> Data is NOT normal (reject H0)")

    # 3. Plot histogram + KDE
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    sns.histplot(df[col], kde=True)
    plt.title(f"Histogram + KDE of {col}")

    # 4. Q-Q plot
    plt.subplot(1,2,2)
    probplot(df[col], dist="norm", plot=plt)
    plt.title(f"Q-Q Plot of {col}")

    plt.tight_layout()
    plt.show()

cols = ["voltage_kv", "Length_km", "num_towers"]

# Percentile thresholds (you can adjust 1–99 or 2–98)
lower_p = 1
upper_p = 99

print("Outlier removal summary:\n")

for col in cols:
    # Calculate thresholds
    lower_bound = np.percentile(df[col], lower_p)
    upper_bound = np.percentile(df[col], upper_p)

    print(f"{col}:")
    print(f"  Lower {lower_p}th percentile = {lower_bound}")
    print(f"  Upper {upper_p}th percentile = {upper_bound}")

    # Filter the dataframe
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

    print(f"  Remaining rows after removing outliers: {len(df)}\n")

# Final shape
print("\nFinal DataFrame shape:", df.shape)

df.head()

df.to_csv("preprocessed_data_power_grid_100k.csv", index=False)

# Multiple output variables
target_columns = [
    "steel_tonnes",
    "conductor_km",
    "insulators_unit",
    "concrete_cubic_meter",
    "Bus_reactor",
    "Transformers",
    "circuit_breaker"
]

X = df.drop(columns=target_columns)
y = df[target_columns]

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

print("Train:", X_train.shape)
print("Test :", X_test.shape)

model_1 = ExtraTreesRegressor(
    n_estimators=650,        # more trees → reduces noise
    max_depth=None,          # allow full depth → prevents underfitting
    min_samples_split=2,
    min_samples_leaf=2,
     # use more features → reduces randomness
    bootstrap=False,         # ExtraTrees works best WITHOUT bootstrap
    n_jobs=-1,
    random_state=1
)
model_1.fit(X_train, y_train)

import joblib
joblib.dump(model_1, "extratree_model.pkl")

y_pred = model_1.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Overall Multi-Output Model Performance:")
print("MAE :", mae)
print("MSE :", mse)
print("RMSE:", rmse)
print("R²  :", r2)

print("\nTarget-wise Performance:\n")

for i, col in enumerate(target_columns):
    mae_t = mean_absolute_error(y_test[col], y_pred[:, i])
    mse_t = mean_squared_error(y_test[col], y_pred[:, i])
    rmse_t = np.sqrt(mse_t)
    r2_t = r2_score(y_test[col], y_pred[:, i])

    print(f"{col}:")
    print(f"  MAE : {mae_t}")
    print(f"  RMSE: {rmse_t}")
    print(f"  R²  : {r2_t}")
    print("-" * 35)

y_test

y_pred[0]

sample_X = X_test.iloc[[10]]
sample_y_actual = y_test.iloc[[10]]

sample_y_pred = model_1.predict(sample_X)

import pandas as pd

sample_y_pred_df = pd.DataFrame(sample_y_pred, columns=y_test.columns)

comparison = pd.concat([sample_y_actual.reset_index(drop=True),
                        sample_y_pred_df], axis=1)

comparison.columns = [f"{col}_actual" for col in sample_y_actual.columns] + \
                     [f"{col}_pred" for col in sample_y_pred_df.columns]

comparison

sample_y_pred_df

sample_X = X_test.sample(5, random_state=42)
sample_y_actual = y_test.loc[sample_X.index]

sample_y_pred = model_1.predict(sample_X)
sample_y_pred_df = pd.DataFrame(sample_y_pred, columns=y_test.columns)

pd.concat([sample_y_actual.reset_index(drop=True),
           sample_y_pred_df], axis=1)

import pandas as pd
import joblib
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline


# -----------------------------------------------------
# 1) CORRECTED RAW FEATURE SCHEMA
# -----------------------------------------------------

categorical_features = [
    "project_type",
    "region",
    "soil_type",
    "terrain_type",
    "circuit_type",
    "conductor_type"
]

numerical_features = [
    "voltage_kv",
    "Length_km",
    "num_towers"
]

# targets (NOT used in pipeline)
target_features = [
    "steel_tonnes",
    "conductor_km",
    "insulators_unit",
    "concrete_cubic_meter",
    "Bus_reactor",
    "Transformers",
    "circuit_breaker"
]




# -----------------------------------------------------
# 2) LOAD THE 6 OHE ENCODERS
# -----------------------------------------------------

ohe_project_type = joblib.load("/content/OHE_project_type.pkl")
ohe_region       = joblib.load("/content/OHE_region.pkl")
ohe_soil         = joblib.load("/content/OHE_soil.pkl")
ohe_terrain      = joblib.load("/content/terrain_encoded.pkl")
ohe_circuit      = joblib.load("/content/OHE_circuit.pkl")
ohe_conductor    = joblib.load("/content/OHE_conductor.pkl")

# -----------------------------------------------------
# 3) BUILD THE PREPROCESSOR
# -----------------------------------------------------
preprocessor = ColumnTransformer(
    transformers=[
        ("project_type_ohe", ohe_project_type, ["project_type"]),
        ("region_ohe",       ohe_region,       ["region"]),
        ("soil_ohe",         ohe_soil,         ["soil_type"]),
        ("terrain_ohe",      ohe_terrain,      ["terrain_type"]),
        ("circuit_ohe",      ohe_circuit,      ["circuit_type"]),
        ("conductor_ohe",    ohe_conductor,    ["conductor_type"]),
        ("numerical",        "passthrough",     numerical_features)
    ]
)



# -----------------------------------------------------
# 4) LOAD TRAINED MODEL
# -----------------------------------------------------
model = joblib.load("/content/extratree_model.pkl")

# -----------------------------------------------------
# 5) FINAL PIPELINE
# -----------------------------------------------------
full_pipeline = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("model", model)
    ]
)

# -----------------------------------------------------
# 6) SAVE PIPELINE
# -----------------------------------------------------
#joblib.dump(full_pipeline, "full_ml_pipeline.pkl")
print("Final pipeline saved as full_ml_pipeline.pkl")

from google.colab import drive
drive.mount('/content/drive')

import shutil

shutil.make_archive("full_ml_pipeline", 'zip', ".", "/content/full_ml_pipeline.pkl")



import shutil

# Source path: where your file currently is
source = '/content/full_ml_pipeline.pkl'

# Destination path: where you want it in Drive
destination = '/content/drive/MyDrive/full_ml_pipeline.pkl'

# Copy the file
shutil.copy(source, destination)

print("Pipeline uploaded to Drive successfully!")

import pandas as pd
import joblib
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

# -----------------------------------------------------
# 1) FEATURE SCHEMA
# -----------------------------------------------------
categorical_features = [
    "project_type",
    "region",
    "soil_type",
    "terrain_type",
    "circuit_type",
    "conductor_type"
]

numerical_features = [
    "voltage_kv",
    "Length_km",
    "num_towers"
]

target_features = [
    "steel_tonnes",
    "conductor_km",
    "insulators_unit",
    "concrete_cubic_meter",
    "Bus_reactor",
    "Transformers",
    "circuit_breaker"
]

# -----------------------------------------------------
# 2) LOAD DATASET
# -----------------------------------------------------
df = pd.read_csv("/content/without_OHE_preprocessed_data_power_grid_100k.csv")   # ⬅ change file path if needed

# -----------------------------------------------------
# 3) SPLIT INTO X (features) AND y (label)
# -----------------------------------------------------
X = df[categorical_features + numerical_features]
y = df[target_features]   # multi-output regression

# -----------------------------------------------------
# 4) TRAIN/TEST SPLIT
# -----------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -----------------------------------------------------
import joblib
import pandas as pd
#pipeline = joblib.load("/content/drive/MyDrive/full_ml_pipeline.pkl")

test_data = pd.DataFrame([{
    "project_type": "Transmission Line",
    "region": "South",
    "soil_type": "Rocky",
    "terrain_type": "Plains",
    "voltage_kv": 132,
    "circuit_type": "Single Circuit",
    "conductor_type": "ACSR Panther",
    "Length_km": 71.79,
    "num_towers": 282
}])

# -----------------------------------------------------
# 3) MAKE PREDICTIONS
# -----------------------------------------------------
full_pipeline.fit(X_train, y_train)

pipeline

print(type(ohe_project_type))
print(type(ohe_region))
print(type(ohe_soil))
print(type(ohe_terrain))
print(type(ohe_circuit))
print(type(ohe_conductor))

test_data = pd.DataFrame([{
    "project_type": "Transmission Line",
    "region": "South",
    "soil_type": "Rocky",
    "terrain_type": "Plains",
    "voltage_kv": 132,
    "circuit_type": "Single Circuit",
    "conductor_type": "ACSR Panther",
    "Length_km": 71.79,
    "num_towers": 282
}])

# -----------------------------------------------------
# 3) MAKE PREDICTIONS
# -----------------------------------------------------
full_pipeline.predict(test_data)

joblib.dump(full_pipeline, "full_ml_pipeline_1.pkl")

import shutil

# Source path: where your file currently is
source = '/content/full_ml_pipeline_1.pkl'

# Destination path: where you want it in Drive
destination = '/content/drive/MyDrive/full_ml_pipeline_1.pkl'

# Copy the file
shutil.copy(source, destination)

print("Pipeline uploaded to Drive successfully!")

